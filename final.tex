\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage[inline]{enumitem}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{cleveref}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\title{A Survey on Differential Privacy}

\author{
Achal Dave \\
\texttt{achald@cs.cmu.edu} \\
\And
Jingyan Wang \\
\texttt{jingyanw@cs.cmu.edu} \\
\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\input{commands}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle
%
%We also plan to survey and read a few more papers. The options include:
%\begin{itemize}
%\item Privacy attack approaches when databases are not protected by differential privacy, \emph{e.g.}~\cite{ganta2008composition}.
%\item Further parts in the book.
%\item Relevant papers from recent conferences.


\section{Introduction}
The growth of the Internet and electronic databases has led to an increasing
concern about the privacy of user information. These databases facilitate
researchers to analyze patterns from large groups of people and develop machine
learning algorithms, such as identifying trends or recommending movies. Although
increasingly useful, these methods come at the cost of user privacy: as we ask
questions about a large dataset of users' information, we receive information of
not only the group, but also of individual users. Prior work has
theoretically~\cite{ganta2008composition} and
empirically~\cite{sweeney1997weaving} demonstrated that even anonymous datasets
can reveal sensitive details about individuals. For example,
\cite{narayanan2008robust} has shown that individual subscribers in the Netflix
Prize dataset can be accurately identified with a minimal amount of auxiliary
public information.

Differential privacy offers a formal paradigm which guarantees provable privacy
for users, while allowing queries that reveal aggregated statistics about the
dataset as a whole. Revisiting machine learning problems with this notation of
differential privacy has recently given rise to a number of new algorithms and
theoretical guarantees.

We survey the prior works on differential privacy, with an emphasis on empirical
risk minimization (ERM). \Cref{overview} introduces the fundamental
concepts and basic properties. \Cref{section:erm} discusses two
algorithms for empirical risk minimization and considers the tradeoffs between
privacy guarantees and generalization performance. The parameter tuning
technique and specific examples of ERM (\emph{e.g.} logistic regression, SVM)
are also discussed.

\section{Overview}\label{overview}
The notion of differential privacy~\cite{dwork2006calibrating} was first
developed by Dwork \emph{et al} as follows.

%% CLEAN-UP START FROM HERE

We consider datasets that are collections of records from some universe $\Dc$.
For example, if we want to collect a dataset where we ask people to say whether
they smoke or not, the universe of responses is $\Dc = \{0, 1\}$. Further,
denote $\Dc^n$ to be the space of datasets containing $n$ entries from $\Dc$.

We would like to answer \textit{queries} from datasets while preserving some
notion of privacy. We denote $\Mc$ to be a mechanism for answering
queries. To preserve privacy, we want the response of this mechanism
to have a similar distribution regardless of any one individual's response.
That is, we want that for two datasets $D, D'$ that differ in only one
individual (denoted $D \sim D'$), the distribution of $\Mc(D)$ is close to
$\Mc(D')$.

With this notation, we can formalize $(\epsilon, \delta)$-differential
privacy~\cite{dwork2014algorithmic} as follows:

\begin{definition}{(Differential Privacy)}
A random mechanism $\Mc$ is $(\epsilon, \delta)$-differentially private
if for all $\Sc \subseteq \textup{Range}(\Mc)$ and for all
datasets $D, D' \in \Dc^n$ such that $D \sim D'$.
\begin{align*}
P(\Mc(D) \in \Sc) \leq \exp{(\epsilon)} P(\Mc(D') \in \Sc) + \delta
\end{align*}
\end{definition}

A simple, intuitive way of ensuring differential privacy is to add noise to the
output of queries. Suppose a query
\[ q(D) : \Dc^n \rightarrow \mathbb{R}^k \]
is requested. Then, instead of returning $q(D)$, we can return
$q(D) + (Y_1, \dots, Y_k)$, where $Y_i$ is noise drawn from some distribution.
However, the noise must depend, in some sense, on how much $q$ can change for
values of $D, D'$ close to each other.

We can formalize this as follows. Define $\Delta q$, the $L_1$-sensitivity of
$q$, as follows:
\begin{align*}
\Delta q = \max_{D \sim D'} \norm{q(D) - q(D')}_1,
\end{align*}

If $q$ is highly sensitive, then we will need to add more noise to prevent an
adversary from distinguishing $D$ or $D'$ based on $q(D)$ and $q(D')$. It turns
out that we can use what is known as the Laplace mechanism to do this.

\begin{definition}{(Laplace Mechanism)}
For any function $q : \Dc^n \rightarrow \Rb^k$ with $L_1$-sensitivity
$\Delta q$, the Laplace mechanism is defined as
\begin{align*}
\Mc_L(D, q(\cdot), \epsilon) = q(D) + (Y_1, \dots, Y_k)
\end{align*}
where $Y_i$ are drawn i.i.d. from Lap($\Delta q / \epsilon$).
\label{def:laplace_mechanism}
\end{definition}

It can be shown that this mechanism provides $(\epsilon, 0)$ differential
privacy. Note that this mechanism adds noise to the response of the query.

In some cases, adding noise to the response of a query can drastically lower
the utility of the response. In this case, we can instead consider the
exponential mechanism, which allows specifying a utility function for responses.
Let $\Rc$ be the space of possible responses for a given query. Then, we can
define define a utility function $u : \Dc^n \times \Rc \rightarrow \Rb$,
which assigns a utility to a response for a given database. Similar to the
sensitivity of $q$, let's define the sensitivity of $u$:
\begin{align*}
\Delta u \triangleq \max_{r \in \Rc} \max_{D \sim D'} \abs{u(D, r) - u(D', r)}
\end{align*}

Then, the exponential mechanism is defined as follows:
\begin{definition}{(Exponential Mechanism)}
For any utility function $u : \Dc^n \times \Rc \rightarrow \Rb$ with $L_1$
sensitivity $\Delta u$, the exponential mechanism $\Mc_E(D, u, \Rc)$ outputs an
element $r \in \Rc$ with probability proportional to $\exp{(\frac{\epsilon u(D,
r)}{2 \Delta u})}$.
\label{def:exponential_mechanism}
\end{definition}

Unlike the Laplace mechanism, the exponential mechanism doesn't add arbitrary
noise to the response; instead, it randomly selects \textit{which} of a set of
responses to return, so that, with high probability, the utility is not
drastically reduced. It can be shown that the exponential mechanism preserves
$(\epsilon, 0)$ differential privacy.

\section{Differentially-Private Empirical Risk Minimization (ERM)}\label{section:erm}
In this section, we provide and analyze the differentially-private algorithms for empirical risk minimization (ERM)~\cite{chaudhuri2011erm}.

In supervised machine learning, we aim to learn a mapping $f: \X \rightarrow \Y$
from the training set
$D = \{(x_i, y_i) \in \X \times \Y\}_{i=1}^n$,
where $x_i$'s are the data and $y_i$'s are the labels. In the context of our
notation for differential privacy, our training dataset $D$ is the dataset
we would like to keep private, and lives in the space
$\Dc^n = \X^n \times \Y^n$.

One way to solve for $f$ is through minimizing the empirical risk:
\[ f^* = \argmin_{f\in \F} R(f, D)
       = \argmin_{f\in \F} \frac{1}{n} \sum_{i=1}^n l(f(x_i), y_i) + \Lambda N(f), \]
where $\F$ is a function class. The function $l:\Y \times \Y \rightarrow \R$ is
a loss function. $N(\cdot)$ is a regularizer which penalizes the complexity of
$f$, and $\Lambda$ is a tuning parameter. In the remaining of the report, we
consider the classification problem with a linear predictor $f(x) = f^T x$ where
$f\in \R^d$.

The output $f$ is not differentially private. For example, in SVM, the
hyperplane is highly sensitive to the training examples near the margin. Adding
or removing a training example near the margin can drastically change the
hyperplane, revealing certain properties of these individuals.

The idea is to add noise to perturb the output. In the SVM example, it will then become impossible to identify if certain training examples on the margin contribute to the hyperplane, or merely the noise. We use the Laplace mechanism and exponential mechanism in Section~\ref{overview} as building blocks, and discuss two approaches: (1) \emph{output perturbation}: to obtain the predictor $\hat{f}$ by solving the original ERM and then perturb $\hat{f}$ (2) \emph{objective perturbation}: to perturb the ERM objective $R(f, \Dc)$ and then solve for the predictor $\hat{f}$ from the perturbed objective.

\subsection{Output Perturbation}
In output perturbation, we solve the ERM and perturb the solution using the Laplace mechanism. The procedure is given in Algorithm~\ref{alg:output_perturb}.

\begin{algorithm}[htb]
\begin{algorithmic}[1]
  \Function{Output Perturbation}{dataset $D$, differential privacy $\epsilon$, regularization $\Lambda$}
    \State Solve $\hat{f} = \argmin_f R(f, D)$.
    \State Draw a random vector $b\in \R^d, \norm{b}\sim \text{Lap}(\frac{1}{\beta})$, where $\beta = \frac{n\Lambda \epsilon}{2}$.
    \State Return $f_{priv} = \hat{f} + b$
   \EndFunction
\end{algorithmic}
\caption{Output perturbation}\label{alg:output_perturb}
\end{algorithm}

\subsubsection{Privacy Guarantees}
\begin{theorem}
If $N$ is differentiable and $1$-strongly convex; $l$ is convex and differentiable with $\abs{l'} \le 1$, then Algorithm~\ref{alg:output_perturb} is $\epsilon$-differentially private.
\end{theorem}

\begin{proof}
This method is very similar to the Laplace mechanism in \Cref{def:laplace_mechanism}.

Let $h(D) = \argmin_f R(f, D)$, which is just a query on a dataset that
returns some value. From \Cref{cor:output_perturb_sensitivity}, we know that
$\Delta_2 h$, the $L_2$ sensitivity of $h$, is $\frac{n \Lambda}{2}$. Then,
\Cref{alg:output_perturb} is similar to the Laplace mechanism from
\Cref{def:laplace_mechanism} where the noise is drawn from a
$Lap(\frac{2}{n \Lambda \epsilon})$, except that here we rely on the $L_2$
instead of $L_1$ sensitivity, and we draw the magnitude of the noise vector at
random rather than drawing individual elements at random.

Now, we explicitly prove that this method preserves differential privacy.
Consider two datasets $D$ and $D'$ that differ in the value of one individual.
Let $f_1$ and $f_2$ be the solutions output by non-private ERM for datasets $D$
and $D'$ respectively. Let $g(f | D)$ be the density of the solution output by
\Cref{alg:output_perturb} for dataset $D$, and $g(f | D')$ the same for dataset
$D'$.

Since we draw $\norm{b}$ from Lap$(\frac{1}{\beta})$, then the density for
any fixed $b_0$ is proportional to $\exp(-\beta \norm{b_0})$. Then,
\begin{align*}
\frac{g(f | D)}{g(f | D')}
    &= \frac{
         \exp(-\tfrac{n\Lambda\epsilon_p}{2} \norm{b + f_1})
       }{
         \exp(-\tfrac{n\Lambda\epsilon_p}{2} \norm{b + f_2})
       } \\
    &= \exp(-\tfrac{n\Lambda\epsilon_p}{2} (\norm{b + f_2} - \norm{b + f_1})) \\
    &\leq \exp(-\tfrac{n\Lambda\epsilon_p}{2} \norm{f_2 - f_1})
\end{align*}

From \Cref{cor:output_perturb_sensitivity}, we know that the $L_2$ sensitivity
of $\argmin_f R(f, D) \leq \frac{2}{n \Lambda}$, so that
$\norm{f_2 - f_1} \leq \frac{2}{n \Lambda}$. With this, we have satisfied the
definition of $\epsilon_p$ differential privacy:
\begin{align*}
\frac{g(f | D)}{g(f | D')} &\leq \exp(-\epsilon_p),
\end{align*}
concluding the proof.
\end{proof}

\begin{cor}
The $L_2$ sensitivity of $\argmin_f R(f, D)$ is at most $\frac{2}{n \Lambda}$.
\label{cor:output_perturb_sensitivity}
\end{cor}
\begin{proof}
Let $D = \{(x_1, y_1), \dots, (x_n, y_n)\}$ and
$D' = \{(x_1, y_1), \dots, (x_n', y_n')\}$
be two datasets that differ in the value of the $n$th individual.
Let
\begin{align*}
G(f) &= J(f, D) \\
g(f) &= J(f, D') - J(f, D) \\
     &= \frac{1}{n} (L(y_n' f^T x_n') - L(y_n f^T x_n)).
\end{align*}

By the convexity of $L$ and the $1$-strong convexity of $N(\dot)$, we know that
$J(f, D)$ is $\Lambda-$strongly convex, and therefore, so are $G(f)$ and
$G(f) + g(f)$.
Further,
\begin{align*}
\nabla g(f)
= \frac{1}{n} \left(y_n' L'(y_n' f^T x_n') x_n' - y_n L'(y_n f^T x_n) x_n\right)
\end{align*}

Since $y_i \in [-1, 1], \abs{L'(z)} \leq 1$ for all $z$, and
$\norm{x_i} \leq 1$, we can see that for any $f$,
\begin{align*}
\norm{\nabla g(f)} \leq \frac{2}{n}
\end{align*}

We can now appeal to \Cref{lemma:strong_convex_sensitivity} to conclude the
proof.
\end{proof}

\begin{lemma}
Let $G(f)$ and $g(f)$ be continuous vector-valued functions which are
differentiable at all points. Let $G(f)$ and $G(f) + g(f)$ be $\lambda-$strongly
convex. If $f_1 = \argmin_f G(f)$, and $f_2 = \argmin_f G(f) + g(f)$, then
\begin{align*}
\norm{f_1 - f_2} \leq \frac{1}{\lambda} \max_f \norm{\nabla g(f)}
\end{align*}
\label{lemma:strong_convex_sensitivity}
\end{lemma}

\begin{proof}
TODO(achald): Should we write out this proof?
\end{proof}

\subsubsection{Performance Guarantees}
We establish the performance guarantees in the following notion: we compare the loss induced by the differentially-private output $f_{priv}$ with the loss induced any arbitrary classifier $f_0$. We then prove that with a sufficient number of samples, the loss induced by $f_{priv}$ is smaller or arbitrarily close to the loss induced by $f_0$. The term $\norm{f_0}$ is included in the sample complexity bound.
\begin{theorem} Let $N(f) = \frac{1}{2}\norm{f}^2$, and let $f_0$ be any classifier. Let $L(f) = \Eb_{x, y}\; [l(f^T x, y)]$ be the true loss. Let $l$ be differentiable and continuous, and let $l'$ be $c$-Lipschitz with $\abs{l'} \le 1$. Then, there exists a constant $C$ s.t. when the number of training samples $n$ satisfies
$$n > C\max{\left\{\frac{\norm{f_0}^2 \log{\frac{1}{\delta}}}{\epsilon_g^2}, \frac{d\log{\frac{d}{\delta}}\norm{f_0}^2}{\epsilon_g \epsilon}, \frac{d\log{\frac{d}{\delta}}\sqrt{c} \norm{f_0}^2}{\epsilon_g^{\frac{3}{2}}\epsilon}\right\}},$$

the output $f_{priv}$ of Algorithm~\ref{alg:output_perturb} satisfies $\Pb(L(f_{priv}) \le L(f_0) + \epsilon_g) \ge 1 - 2\delta$.
\end{theorem}

\begin{proof}[Proof sketch]
Let $\overline{J}(f) = L(f) + \frac{\Lambda}{2} \norm{f}^2$, and let \begin{align*}
& f_{rtr} = \argmin_f \overline{J}(f)\\
& f^* = \argmin_f J(f, D)
\end{align*}

We decompose the loss as follows and bound the terms separately:
\begin{align*}
L(f_{priv}) -  L(f_0) = [\overline{J}(f_{priv}) - \overline{J}(f_{rtr})] + \left[\overline{J}(f_{rtr}) - \overline{J}(f_0)\right] + \frac{\Lambda}{2} \norm{f_0}^2 - \frac{\Lambda}{2} \norm{f_{priv}}^2
\end{align*}

For the first term, we first bound the difference between the (empirical) objective of $f_*$ and that of the differentially-private solution $f_{priv}$, by the conditions $\norm{x_i} \le 1, \abs{y_i} \le 1$:
\begin{align}
J(f_{priv}, D) - J(f^*, D) \le \frac{8d^2\log^2{(\frac{d}{\delta})} (c + \Lambda)}{\Lambda^2 n^2 \epsilon^2}.\label{thm15:emp}
\end{align}

Then, we apply Theorem 1 of~\cite{sridharan2008objective} and have:
\begin{align*}
\overline{J}(f_{priv}) - \overline{J}(f_{rtr}) & \le 2 [J(f_{priv}, D) - J(f^*, D)] + O(\frac{\log{(\frac{1}{\delta})}}{\Lambda n})\\
& \le  \frac{16d^2\log^2{(\frac{d}{\delta})} (c + \Lambda)}{\Lambda^2 n^2 \epsilon^2} + O(\frac{\log{(\frac{1}{\delta})}}{\Lambda n})
\end{align*}
For the second term, by definition of $f_{rtr}$, we have $\overline{J}(f_{rtr}) - \overline{J}(f_0) \le 0$.

Setting $\Lambda = \frac{\epsilon_g}{\norm{f_0}^2}$ in the three terms, we obtain:
\begin{align*}
L(f_{priv}) - L(f_0) \le  \frac{16\norm{f_0}^4 d^2\log^2{(\frac{d}{\delta})} (c +\frac{\epsilon_g}{\norm{f_0}^2})}{n^2 \epsilon_g^2\epsilon^2} + O(\norm{f_0}^2\frac{\log{(\frac{1}{\delta})}}{n\epsilon_g}) + \frac{\epsilon_g}{2}.
\end{align*}

Equating the RHS with $\epsilon_g$ and solving for $n$ complete the proof.
\end{proof}

\subsection{Objective Perturbation}
Alternative to perturbing the solution of the algorithm, we can also perturb the objective before optimization. The procedure is given in Algorithm~\ref{alg:obj_perturb}.

\begin{algorithm}[htb]
\begin{algorithmic}[1]
  \Function{Objective Perturbation}{dataset $D$, differential privacy $\epsilon$, regularization $\Lambda$}
    \State Set $\epsilon' = \epsilon - \log{(1 + \frac{2c}{n\Lambda} + \frac{c^2}{n\Lambda^2})}$.
    \If{$\epsilon' > 0$}
	\State $\Delta = 0$.
    \Else
	\State $\epsilon' = \frac{\epsilon}{2}$.
	\State $\Delta = \frac{c}{n(e^{\frac{\epsilon}{4}} - 1)} = \lambda$.
    \EndIf
    \State Draw a random vector $b\in \R^d, \norm{b}\sim \text{Lap}(\frac{1}{\beta})$, where $\beta = \frac{\epsilon'}{2}$.
    \State Return $f_{p} = \argmin_f R(f, D) + \frac{1}{n} b^T f + \frac{1}{2}\Delta\norm{f}^2 $
   \EndFunction
\end{algorithmic}
\caption{Objective perturbation}\label{alg:obj_perturb}
\end{algorithm}

We note that (1) there is an additional regularization term $\frac{1}{2}\Delta \norm{f}_2^2$, and that
(2) a slack of $\log{(1 + \frac{2c}{n\Lambda} + \frac{c^2}{n^2\Lambda^2})}$ is introduced. When proving $\epsilon$-differential privacy, we split the density ratio into two factors, and bound them respectively with $e^{\epsilon'}$ and $e^{\epsilon - \epsilon'}$. The two cases in Line 3-7 are needed to ensure that $\epsilon' > 0$.

\subsubsection{Privacy Guarantees}
\begin{theorem}
If $N$ is doubly differentiable and $1$-strongly convex; $\ell$ is convex and doubly differentiable with $\abs{\ell'} \le 1, \abs{\ell''} \le c$, then Algorithm~\ref{alg:obj_perturb} is $\epsilon$-differentially private.
\end{theorem}

\begin{proof}[Proof sketch]
Since $J(f, D) + \frac{1}{2}\Delta \norm{f}_2^2$ is strongly-convex, we take the gradient and set it to 0:
\begin{align}
& \del [J(f, D) +  \frac{1}{2}\Delta \norm{f}_2^2]= \frac{1}{n}\sum_{i=1}^n \del \ell(f^T x_i, y_i) + \frac{1}{n} b+ \Lambda \del N(f) + \Delta f  \triangleq 0\nonumber\\
\Rightarrow \quad & b =  -\sum_{i=1}^n \del \ell(f^Tx_i, y_i) -n \Lambda \del N(f) - n\Delta f\nonumber\\
& =  -\sum_{i=1}^n y_i \ell''(y_i f^T x) x_i - n\Lambda \del N(f) - n\Delta f\label{thm9:b}
\end{align}

This equation defines a mapping from any vector $f$ to a unique $b$. On the other hand, since the objective is strongly-convex, for any fixed $b$, there exists a unique $f$. Hence, there is a bijection between $b$ and $f$.

Then, we write the density ratio of the output $f_{priv}$ in terms of the density ratio of the noise $b$, using transformation of variables:
\begin{align}
\frac{g(f_{priv} | D)}{g(f_{priv} | D')} = \frac{\mu(b | D)}{\mu(b' | D')} \cdot \frac{\abs{\det(J(f_{priv} \rightarrow b | D))}^{-1}}{\abs{\det(J(f_{priv} \rightarrow b' | D'))}^{-1}}, \label{thm9:main}
\end{align}
where $J$ denotes the Jacobian matrix of the mapping.

From~\eqref{thm9:b}, we analytically compute and bound the ratio of the Jacobian matrices:
\begin{align*}\frac{\abs{\det(J(f_{priv} \rightarrow b | D))}^{-1}}{\abs{\det(J(f_{priv} \rightarrow b' | D'))}^{-1}} \le \left(1 + \frac{c}{n(\Lambda + \Delta)}\right)^2
\end{align*}

We want to bound this term by $e^{\epsilon - \epsilon'}$, so in the first case, we directly set $\epsilon' = \epsilon  - \log{(1 + \frac{c}{n\Lambda})^2}$, and $\Delta = 0$ if $\epsilon' > 0$. In the second case, we address the case when $\epsilon'$ becomes negative, and reset $\epsilon' = \frac{\epsilon}{2}$ and $\Delta = \frac{c}{n(e^{\frac{\epsilon}{4}}- 1)}- \Lambda$. We can verify that falling into the second case implies $\Delta > 0$. Hence,
\begin{align}
\frac{\abs{\det(J(f_{priv} \rightarrow b | D))}^{-1}}{\abs{\det(J(f_{priv} \rightarrow b' | D'))}^{-1}} \le e^{\epsilon - \epsilon'} \label{thm9:ratio_jacobian}
\end{align}
Next, we bound the density of the ratio of the noise. From~\eqref{thm9:b}, we have
\begin{align*}
& b' - b = y_n \ell'(y_n f^T x_n) x_n - y_n' \ell'(y_n f^T x_n') x_n'
\end{align*}

From the conditions $\abs{\ell'} \le 1, \abs{y_i} \le 1, \norm{x_i} \le 1$,
\begin{align*}
& \norm{b} - \norm{b'} \le \norm{b - b'} \le 2
\end{align*}

Then, \begin{align}
 \frac{\mu(b | D)}{\mu(b' | D')} = \frac{e^{-\frac{\epsilon' \norm{b}}{2}}}{e^{\frac{-\epsilon' \norm{b'}}{2}}}\le e^{\frac{\epsilon'(\norm{b} - \norm{b'})}{2}} \le e^{\epsilon'} \label{thm9:ratio_noise}
\end{align}
We complete the proof by plugging the two parts,~\eqref{thm9:ratio_jacobian} and~\eqref{thm9:ratio_noise}, into~\eqref{thm9:main}.
\end{proof}

\subsubsection{Performance Guarantees}
We follow the same framework and notations as in output perturbation.
\begin{theorem}
Let $f_0$ be any classifier. If $\ell$ is convex, doubly differentiable with derivative $|\ell'| \le 1$ and $|\ell''| \le c$. Then, there exists a constant $C$ s.t. when the number of training samples $n$ satisfies
$$n > C\max{\left\{ \frac{\norm{f_0}^2 \log{\frac{1}{\delta}}}{\epsilon_g^2}, \frac{c\norm{f_0}^2}{\epsilon_g \epsilon}, \frac{d\log{\frac{d}{\delta}} \norm{f_0}}{\epsilon_g\epsilon}\right\}},$$
the output $f_{priv}$ of Algorithm~\ref{alg:obj_perturb} satisfies $P(L(f_{priv}) \le L(f_0) + \epsilon_g) \ge 1 - 2\delta$.
\end{theorem}
\begin{proof}[Proof sketch]
The proof is similar to the output perturbation case, where we decompose the loss into the same three terms. The analysis for $J(f_{priv}, D) - J(f^*, D)$ is different (\emph{cf.} Equation~\eqref{thm15:emp}).
\end{proof}

Comparing Theorem 7 and 9, objective perturbation has a better generalization bound compared to output perturbation.

% TODO: compare the sample complexity for the two methods in word

\section{Parameter Tuning}
The above methods assumed that we were provided with a fixed value of $\Lambda$,
the regularization parameter. In practice, however, it is common to train
multiple models by performing a sweep over a set of values of $\Lambda$, and
choosing the value that performs best on a hold-out set. However, this requires
running the algorithm multiple times on the same dataset, which can compromise
differential privacy by revealing a relationship between $\Lambda$ and the
output. Instead, we need an end to end differentially private method for
choosing $\Lambda$.

One option to is to tune $\Lambda$ on a public dataset where privacy is not of
concern. Then, using this tuned $\Lambda$, we can use \Cref{alg:output_perturb}
or \Cref{alg:obj_perturb} to train a classifier in a differentially private
manner. However, such a dataset is not always available. In this case, we can
use the exponential mechanism to optimize over possible values of $\Lambda$, as
in \Cref{def:exponential_mechanism}.

\begin{algorithm}[htb]
\begin{algorithmic}[1]
  \Function{Privacy-preserving Parameter Tuning}{$D, \{\Lambda_1, \dots, \Lambda_m\}, \epsilon_p$}
    \State Divide $D$ into $m + 1$ equal portions $D_1, \dots D_{m+1}$, each of
           size $\tfrac{\abs{D}}{m + 1}$.
    \State For each $i = 1, \dots, m$, use a privacy-preserving algorithm
           (such as \Cref{alg:output_perturb} or \Cref{alg:obj_perturb}) on
           $D_i$ with parameters $\Lambda_i$ and $\epsilon_p$ to retrieve an
           output $f_i$.
    \State Evaluate $z_i$, the number of mistakes made by $f_i$ on $D_{m+1}$.
    \State Return $f_i$ with probability
           \begin{align*}
           \frac{e^{-\epsilon_p z_i / 2}}{\sum_{j=1}^m e^{-\epsilon_p z_j / 2}}
           \end{align*}
   \EndFunction
\end{algorithmic}
\caption{Parameter tuning}\label{alg:parameter_tuning}
\end{algorithm}

This is the approach taken by\Cref{alg:parameter_tuning}, splits the dataset $D$
into $m+1$ subsets, and uses the last subset as a hold out dataset. Crucially,
the algorithm defines a \textit{utility} function $u(f_i) = -z_i$ as the
negation of the number of mistakes made by the $i$th minimizer. Notice that the
$L_1$ sensitivity of $u$, $\Delta u$, is $1$.  From this view, the algorithm
uses the exponential mechanism and outputs an $f_i$ with probability
proportional to $\exp(\tfrac{\epsilon_p u(D, r)}{2 \Delta u})$ as in
\Cref{def:exponential_mechanism}, thus satisfying differential privacy.

Further, \cite{chaudhuri2011erm} show that the classifier output by
the above algorithm does not make many more mistakes than the best of the $m$
classifiers, as in \Cref{thm:parameter_tuning}.

\begin{theorem}
Let $z_{\text{min}} = \min_i z_i$, and let $z$ be the number of mistakes made on
$D_{m+1}$ by the classifier output from \Cref{alg:parameter_tuning}. Then, with
probability $1 - \delta$,
\begin{align*}
z \leq z_{\text{min}} + \frac{2 \log(m/\delta)}{\epsilon_p}
\end{align*}
\label{thm:parameter_tuning}
\end{theorem}

\section{Examples}
In this section, we briefly give examples on how standard learning algorithms fit in this framework.
	\subsection{Logistic Regression}
	In regularized logistic regression, we have
	\begin{align*}
	& N(f) = \frac{1}{2} \norm{f}^2\\
	& l(z) = \log{(1 + e^{-z})}
	\end{align*}

	$l$ is continuous and doubly differentiable with $c = \frac{1}{4}$.
	\subsection{SVM}
	In $L_2$-regularized support vector machines,
	\begin{align*}
	& N(f) = \frac{1}{2} \norm{f}^2\\
	& l(z) = \max{(0, 1 - z)}
	\end{align*}

	This loss function is not differentiable. There are two ways to address the issue:
	\begin{enumerate}[label=(\roman*)]
		\item Use a doubly differentiable surrogate function.
		\item Use a differentiable surrogate function.
		For example, the Huber loss is only piecewise doubly differentiable. We slightly modify the proofs by arguing that the set in which the function is not differentiable has measure $0$, and hence does not affect the density ratio in the definition of differential privacy.
	\end{enumerate}

\section{Discussions}

We have discussed the definition of differential privacy as well as a couple of
ways to ensure differential privacy for general queries. Further, we showed the
applications of these mechanisms for empirical risk minimization, so that we
are able to train classifiers on datasets in a differentially private manner.
As we look further into providing privacy in the real world, we must consider
additional challenges as well as alternative definitions of privacy.

\textbf{Local Differential Privacy.} So far, we have considered differential
privacy where a trusted authority has access to the true dataset. This authority
can then receive a query, run a differentially private algorithm on the dataset,
and provide some perturbed result of the query to the statistician. In the real
world, however, individuals may be unwilling to sacrifice their privacy to
\textit{any} trusted authority. How can an authority receive some information
from individuals that maintains their privacy from the authority? This is a
stronger claim than differential privacy -- intuitively, if the data is private
from the authority, the authority cannot provide any more privacy sacrificing
information to a statistician who queries the dataset. \cite{duchi2013local}
discuss this setting of \textit{local} differential privacy, and, critically,
show that for many statistical procedures, the effective sample size under
$\alpha$-local differential privacy with $n$ data points in $d$ dimensions is
reduced to $n \alpha^2 / d$ from $n$, where $\alpha < 1$. This reduction can lead to drastically poor
estimation results, and shows that providing local differential privacy can be
very costly in terms of the accuracy of query results.

\textbf{Implementation Challenges.} We have seen algorithms that provide
strong theoretical guarantees on both the privacy afforded to the individuals in
a dataset as well and on the accuracy of the output. A number of packages have
been implemented that expose the Laplace mechanism for end users, such as
Airavat \cite{mohan2012gupt} and GUPT \cite{roy2010airavat}. In practice,
however, implementation challenges can sacrifice privacy despite theoretical
guarantees. For example, the details of the floating point standard can cause
severe issues in privacy guarantees. In particular, the non-uniform spacing of
the doubles causes issues in the support of the perturbed output values of the
Laplace mechanism - the algorithm's output may have support on certain values
when run on one dataset, but not on another dataset, violating differential
privacy.  \cite{mironov2012significance} describes this issue, and shows that it
is possible to consistently violate privacy using current implementations of
Laplace mechanisms, and shows how to overcome these challenges.

\section{Conclusion}
Despite these challenges facing privacy, it is possible to
provide strong theoretical guarantees in the case of standard differential
privacy for both privacy and performance. This survey, in addition to providing
an overview of differential privacy, discusses methods for performing empirical
risk minimization while maintaining privacy. It remains unclear whether these
methods are optimal, and minimax rates for ERM under differential privacy remain
elusive. However, the methods discussed here show that it is possible to perform
private ERM without significantly sacrificing performance, and suggests that
differential privacy in conjunction with other machine learning problems can
provide a useful trade-off between privacy and utility. Further work in
combining machine learning algorithms with differential privacy will hopefully
alleviate this issue, making it easier for dataset owners (such as companies) to
share useful information about populations with outside groups while maintaining
privacy.

{\small
\bibliographystyle{plain}
\bibliography{proposal}
}
\appendix
\section*{Appendix}
% Division of work:
%We both read the papers above. AD wrote Section 2; JW wrote Section 3.

\end{document}
