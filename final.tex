\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage[inline]{enumitem}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{cleveref}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\title{A Survey on Differential Privacy}

\author{
Achal Dave \\
\texttt{achald@cs.cmu.edu} \\
\And
Jingyan Wang \\
\texttt{jingyanw@cs.cmu.edu} \\
\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\input{commands}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

%\section{Logistics}
%
%\begin{enumerate}[label=(\roman*)]
%\item Introduction:
%
%Please see Section 1 and 2.
%\item What we have done so far:
%
%We read part of the book~\cite{dwork2014algorithmic} for the background and introduction. We read~\cite{chaudhuri2011erm} on differential privacy in machine learning.
%\item What remains to be done:
%
%We plan to scrutinize and summarize the proof ideas in~\cite{chaudhuri2011erm}.
%
%We also plan to survey and read a few more papers. The options include:
%\begin{itemize}
%\item Privacy attack approaches when databases are not protected by differential privacy, \emph{e.g.}~\cite{ganta2008composition}.
%\item Further parts in the book.
%\item Relevant papers from recent conferences.
%\end{itemize}
%\item Division of work:
%
%We both read the papers above. AD wrote Section 2; JW wrote Section 3.
%\end{enumerate}

\section{Introduction}
The growth of the Internet and electronic databases has led to an
increasing concern about the privacy of users' information. These databases
enable researchers to learn information, such as
identifying new trends or learning to recommend movies, from large
groups of people. Although increasingly useful, these methods come at the cost
of users' privacy: the more questions we ask about a large dataset of users'
information, the more information we receive about each individual user's
information. Prior work \cite{narayanan2008robust, sweeney1997weaving,
ganta2008composition} has shown that even anonymous datasets can reveal
sensitive details about individuals.

Differential privacy aims to provide provable privacy for users with high probability, while allowing queries that reveal details about the dataset as a whole. Combining differential privacy and machine learning methods have been giving rise to new algorithms and new theoretical guarantees.

In the project, we survey the fundamental concepts about differential privacy, understand differentially-private algorithms and their guarantees, and trade-offs that are imposed by the differential privacy property.

% Reading List
% Some broad motivation for the need for differential privacy. \cite{narayanan2008robust}
% Fundamentals of differential privacy from excerpts of a book. \cite{dwork2014algorithmic}
% A survey of differential privacy results. \cite{dwork2008differential}
% Applications of differential privacy in empirical risk minimization, which include logistic regression and SVM.~\cite{chaudhuri2011differentially} If time permits, papers from recent conferences.

\section{Overview}\label{overview}
We first formalize the notion of differential privacy, following~\cite{dwork2014algorithmic}.

We consider datasets that are collections of records from some
universe $\mathcal{X}$. Further, we can represent such a dataset $x$ as a
\textit{histogram} of records, so that $x \in \mathbb{N}^{\abs{\X}}$. If we want
to collect a dataset where we ask people to say whether they smoke or not, the
universe of responses is $\mathcal{X} = \{0, 1\}$, and the histogram
representation is simply a count of the number of people who smoke and the
number of people who do not smoke.

We would like to answer \textit{queries} from datasets while preserving some
notion of privacy. We let $\mathcal{M}$ be a mechanism for answering
queries. To preserve privacy, we want the response of this mechanism
to have a similar distribution regardless of any one individual's response.
That is, we want that for two datasets $x, y$ that differ in only one
individual (that is, $\norm{x - y}_1 \leq 1$), the distribution of
$\mathcal{M}(x)$ is close to $\mathcal{M}(y)$.

With this notation, we can formalize $(\epsilon, \delta)$-differential privacy.

\begin{definition}{(Differential Privacy)}
A random mechanism $\mathcal{M}$ is $(\epsilon, \delta)$-differentially private
if for all $\mathcal{S} \subseteq \textrm{Range}(\mathcal{M})$ and for all
datasets $x, y \in \mathbb{N}^{\abs{\mathcal{X}}}$
such that $\norm{x - y}_1 \leq 1$,
\begin{align*}
P(\mathcal{M}(x) \in \mathcal{S}) \leq \exp{(\epsilon)} P(\mathcal{M}(y) \in \mathcal{S}) + \delta
\end{align*}
\end{definition}

A simple, intuitive way of ensuring differential privacy is to add noise to the
output of queries. Suppose a query
\[ f(x) : \mathbb{N}^{\abs{\mathcal{X}}} \rightarrow \mathbb{R}^k \]
is requested. Then, instead of
returning $f(X)$, we can return $f(x) + (Y_1, \dots, Y_k)$, where $Y_i$ is noise
drawn from some distribution. However, the noise must depend, in some sense, on
how much $f$ can change for values of $x, y$ close to each other.

We can formalize this as follows. Define $\Delta f$, the $L_1$-sensitivity of
$f$, as follows:
\begin{align*}
\Delta f = \max_{\substack{x, y \in \Nb^{\abs{\Xc}} \\ \norm{x - y}_1 = 1}}
                \norm{f(x) - f(y)}_1
\end{align*}

If $f$ is highly sensitive, then we will need to add more noise to prevent an
adversary from distinguishing $x$ or $y$ based on $f(x)$ and $f(y)$. It turns
out that we can use what is known as the Laplace mechanism to do this.

\begin{definition}{(Laplace Mechanism)}
For any function $f : \Nb^{\abs{\Xc}} \rightarrow \Rb^k$ with $L_1$-sensitivity
$\Delta f$, the Laplace mechanism is defined as
\begin{align*}
\Mc_L(x, f(\cdot), \epsilon) = f(x) + (Y_1, \dots, Y_k)
\end{align*}
where $Y_i$ are i.i.d. drawn from Lap($\Delta f / \epsilon$).
\label{def:laplace_mechanism}
\end{definition}

It can be shown that this mechanism provides $(\epsilon, 0)$ differential
privacy. Note that this mechanism adds noise to the response of the query.

In some cases, adding noise to the response of a query can drastically lower
the utility of the response. In this case, we can instead consider the
exponential mechanism, which allows specifying a utility function for responses.

Define a utility function $u : \Nb^{\abs{\Xc}} \times \Rc \rightarrow \Rb$,
which assings a utility to a response for a given database. Similar to the
sensitivity of $f$, let's define the sensitivity of $u$:
\begin{align*}
\Delta u \triangleq \max_{r \in \Rc} \max_{x, y : \norm{x - y}_1 \leq 1} \abs{u(x, r) - u(y, r)}
\end{align*}

For this utility function, the exponential mechanism is defined as follows
\begin{definition}{(Exponential Mechanism)}
The exponential mechanism $\Mc_E(x, u, \Rc)$ outputs an element $r \in \Rc$ with
probability proportional to $\exp{(\frac{\epsilon u(x, r)}{2 \Delta u})}$.
\end{definition}

Unlike the Laplace mechanism, the exponential mechanism doesn't add arbitrary
noise to the response; instead, it simply randomizes \textit{which} of a set of
responses to return, so that the utility is not drastically reduced with high
probability. It can be shown that the exponential mechanism preserves
$(\epsilon, 0)$ differential privacy.

\section{Privacy-Preserving Machine Learning}
In this section, we discuss how to preserve differential privacy during the learning procedure~\cite{chaudhuri2011erm}.

We consider the standard supervised learning setting, where we aim to learn a mapping $f: \X \rightarrow \Y$ from the training data $D = \{(x_i, y_i) \in \X \times \Y\}_{i=1}^n$. One way to solve for such a function $f$ is through empirical risk minimization (ERM):
$$f^* = \argmin_{f\in \F} R(f, D) \ =\argmin_{f\in \F} \frac{1}{n}\sum_{i=1}^n L(f(x_i), y_i) + \lambda N(f)$$
$\F$ is a function class which $f$ is optimized over. $L:\Y \times \Y \rightarrow \R$ is a loss function. $N(\cdot)$ is a regularizer which penalizes the complexity of $f$. Specifically, we consider the classification problem with a linear predictor $f(x) = f^T x$ where $f\in \R^d$.

The mapping $f$ may reveal certain properties of the training data and lead to privacy leak. Following ideas from Section~\ref{overview}, we add noise to the learning procedure to preserve privacy. We discuss two approaches: (1) \emph{output perturbation}: solve $\hat{f}$ and perturb $\hat{f}$ (2) \emph{objective perturbation}: perturb the objective function $R(f, D)$ and solve for $\hat{f}$ from the perturbed objective.

\subsection{Output Perturbation}
We solve the ERM and perturb the optimizer using the Laplace mechanism. The procedure is given in Algorithm~\ref{alg:output_perturb}.

\begin{algorithm}[htb]
\begin{algorithmic}[1]
  \Function{Output Perturbation}{$D, \epsilon, \lambda$}
    \State Solve $\hat{f} = \argmin_f R(f, D)$.
    \State Draw a random vector $b\in \R^d, \norm{b}\sim \text{Lap}(\frac{1}{\beta})$, where $\beta = \frac{n\lambda \epsilon}{2}$.
    \State Return $f_{p} = \hat{f} + b$
   \EndFunction
\end{algorithmic}
\caption{Output perturbation}\label{alg:output_perturb}
\end{algorithm}

\subsubsection{Privacy Guarantees}
\begin{theorem}
If $N$ is differentiable and $1$-strongly convex; $L$ is convex and differentiable with $\abs{L'} \le 1$, then Algorithm~\ref{alg:output_perturb} is $\epsilon$-differentially private.
\end{theorem}

\begin{proof}
Before touching the details of the proof, it is useful to notice that this method
is very similar to the Laplace mechanism form \Cref{def:laplace_mechanism}.
Let $h(D) = \argmin_f R(f, D)$, which is just a query on a dataset that
returns some value. From \Cref{cor:output_perturb_sensitivity}, we know that
$\Delta_2 h$, the $L_2$ sensitivity of $h$, is $\frac{n \lambda}{2}$. Then,
\Cref{alg:output_perturb} is similar to the Laplace mechanism from
\Cref{def:laplace_mechanism} where the noise is drawn from a
$Lap(\frac{2}{n \lambda \epsilon})$, except that here we rely on the $L_2$
instead of $L_1$ sensitivity, and we draw the magnitude of the noise vector at
random rather than drawing individual elements at random.

Now, we explicitly prove that this method preserves differential privacy.
Consider two datasets $D$ and $D'$ that differ in the value of one individual.
Let $f_1$ and $f_2$ be the solutions output by non-private ERM for datasets $D$
and $D'$ respectively. Let $g(f | D)$ be the density of the solution output by
\Cref{alg:output_perturb} for dataset $D$, and $g(f | D')$ the same for dataset
$D'$.

Since we draw $\norm{b}$ from Lap$(\frac{1}{\beta})$, then the density for
any fixed $b_0$ is proportional to $\exp(-\beta \norm{b_0})$. Then,
\begin{align*}
\frac{g(f | D)}{g(f | D')}
    &= \frac{
         \exp(-\tfrac{n\Lambda\epsilon_p}{2} \norm{b + f_1})
       }{
         \exp(-\tfrac{n\Lambda\epsilon_p}{2} \norm{b + f_2})
       } \\
    &= \exp(-\tfrac{n\Lambda\epsilon_p}{2} (\norm{b + f_2} - \norm{b + f_1})) \\
    &\leq \exp(-\tfrac{n\Lambda\epsilon_p}{2} \norm{f_2 - f_1})
\end{align*}

From \Cref{cor:output_perturb_sensitivity}, we know that the $L_2$ sensitivity
of $\argmin_f R(f, D) \leq \frac{2}{n \Lambda}$, so that
$\norm{f_2 - f_1} \leq \frac{2}{n \Lambda}$. With this, we have satisfied the
definition of $\epsilon_p$ differential privacy:
\begin{align*}
\frac{g(f | D)}{g(f | D')} &\leq \exp(-\epsilon_p),
\end{align*}
concluding the proof.
\end{proof}

\begin{cor}
The $L_2$ sensitivity of $\argmin_f R(f, D)$ is at most $\frac{2}{n \Lambda}$.
\label{cor:output_perturb_sensitivity}
\end{cor}
\begin{proof}
Let $D = \{(x_1, y_1), \dots, (x_n, y_n)\}$ and
$D' = \{(x_1, y_1), \dots, (x_n', y_n')\}$
be two datasets that differ in the value of the $n$th individual.
Let
\begin{align*}
G(f) &= J(f, D) \\
g(f) &= J(f, D') - J(f, D) \\
     &= \frac{1}{n} (L(y_n' f^T x_n') - L(y_n f^T x_n)).
\end{align*}

By the convexity of $L$ and the $1$-strong convexity of $N(\dot)$, we know that
$J(f, D)$ is $\Lambda-$strongly convex, and therefore, so are $G(f)$ and
$G(f) + g(f)$.
Further,
\begin{align*}
\nabla g(f)
= \frac{1}{n} \left(y_n' L'(y_n' f^T x_n') x_n' - y_n L'(y_n f^T x_n) x_n\right)
\end{align*}

Since $y_i \in [-1, 1], \abs{L'(z)} \leq 1$ for all $z$, and
$\norm{x_i} \leq 1$, we can see that for any $f$,
\begin{align*}
\norm{\nabla g(f)} \leq \frac{2}{n}
\end{align*}.

We can now appeal to \Cref{lemma:strong_convex_sensitivity} to conclude the
proof.
\end{proof}

\begin{lemma}
Let $G(f)$ and $g(f)$ be continuous vector-valued functions which are
differentialbe at all points. Let $G(f)$ and $G(f) + g(f)$ be $\lambda-$strongly
convex. If $f_1 = \argmin_f G(f)$, and $f_2 = \argmin_f G(f) + g(f)$, then
\begin{align*}
\norm{f_1 - f_2} \leq \frac{1}{\lambda} \max_f \norm{\nabla g(f)}
\end{align*}
\label{lemma:strong_convex_sensitivity}
\end{lemma}

\begin{proof}
TODO(achald): Should we write out this proof?
\end{proof}

\subsubsection{Performance Guarantees}
We denote $n$ as the number of samples to achieve error $L^* + \epsilon_g$, where $L^*$ is the loss achieved by the predictor $f_0$.

\begin{theorem} (Details omitted for progress report)
$$n > C\max{\left\{\frac{\norm{f_0}^2 \log{\frac{1}{\delta}}}{\epsilon_g^2}, \frac{d\log{\frac{d}{\delta}}\norm{f_0}^2}{\epsilon_g \epsilon}, \frac{d\log{\frac{d}{\delta}}\sqrt{c} \norm{f_0}^2}{\epsilon_g^{\frac{3}{2}}\epsilon}\right\}}.$$
\end{theorem}


\subsection{Objective Perturbation}
Alternative to perturbing the solution of the algorithm, we can also perturb the objective before optimization. The procedure is given in Algorithm~\ref{alg:obj_perturb}.

\begin{algorithm}[htb]
\begin{algorithmic}[1]
  \Function{Objective Perturbation}{$D, \epsilon, \lambda$}
    \State Set $\epsilon' = \epsilon - \log{(1 + \frac{2c}{n\lambda} + \frac{c^2}{n\lambda^2})}$.
    \If{$\epsilon' > 0$}
	\State $\Delta = 0$.
    \Else
	\State $\epsilon' = \frac{\epsilon}{2}$.
	\State $\Delta = \frac{c}{n(e^{\frac{\epsilon}{4}} - 1)} = \lambda$.
    \EndIf
    \State Draw a random vector $b\in \R^d, \norm{b}\sim \text{Lap}(\frac{1}{\beta})$, where $\beta = \frac{\epsilon'}{2}$.
    \State Return $f_{p} = \argmin_f R(f, D) + \frac{1}{n} b^T f + \frac{1}{2}\Delta\norm{f}^2 $
   \EndFunction
\end{algorithmic}
\caption{Objective perturbation}\label{alg:obj_perturb}
\end{algorithm}

We note that \begin{enumerate*}[label=(\roman*)]
\item There is an additional regularization term $\frac{1}{2}\Delta \norm{f}_2^2$.
\item A slack of $\log{(1 + \frac{2c}{n} + \frac{c^2}{n^2\lambda^2})}$ is introduced. When proving $\epsilon$-differential privacy, we split the density ratio into two factors, and bound them respectively with $e^{\epsilon'}$ and $e^{\epsilon - \epsilon'}$. The two cases are needed to ensure that $\epsilon' > 0$.
\end{enumerate*}

\subsubsection{Privacy Guarantees}
\begin{theorem}
If $N$ is doubly differentiable and $1$-strongly convex; $L$ is convex and doubly differentiable with $\abs{L'} \le 1, \abs{L''} \le c$, then Algorithm~\ref{alg:obj_perturb} is $\epsilon$-differentially private.
\end{theorem}

\begin{proof}[Proof sketch]
Since $J(f, D) + \frac{1}{2}\Delta \norm{f}_2^2$ is strongly-convex, we take the gradient and set it to 0:
\begin{align}
\del [J(f, D) +  \frac{1}{2}\Delta \norm{f}_2^2]= & \frac{1}{n}\sum_{i=1}^n \del l(f(x_i), y_i) + \frac{1}{n} b+ \lambda \del N(f) + \Delta f  \triangleq 0\nonumber\\
b =&  -\sum_{i=1}^n \del l(f^Tx_i, y_i) -n \lambda \del N(f) - n\Delta f\nonumber\\
= & -\sum_{i=1}^n y_i l''(y_i f^T x) x_i - n\Lambda \del N(f) - n\Delta f\label{thm9:b}
\end{align}

This equation defines a mapping from any fixed $f$ to a unique $b$. On the other hand, since the objective is strongly-convex, for any fixed $b$, there exists a unique minimizer $f$. Hence, there is a bijection between $b$ and $f$.

Then, we write the density ratio of the output $f_{priv}$ in terms of the density ratio of the noise $b$, using standard transformation of variables:
\begin{align}
\frac{g(f_{priv} | D)}{g(f_{priv} | D')} = \frac{\mu(b | D)}{\mu(b' | D')} \cdot \frac{\abs{\det(J(f_{priv} \rightarrow b | D))}^{-1}}{\abs{\det(J(f_{priv} \rightarrow b' | D'))}^{-1}}, \label{thm9:main}
\end{align}
where $J$ denotes the Jacobian matrix of the mapping.

From~\eqref{thm9:b}, we analytically compute and bound the Jacobian ratio:
\begin{align*}\frac{\abs{\det(J(f_{priv} \rightarrow b | D))}^{-1}}{\abs{\det(J(f_{priv} \rightarrow b' | D'))}^{-1}} \le \left(1 + \frac{c}{n(\Lambda + \Delta)}\right)^2
\end{align*}

We want to bound this term by $e^{\epsilon_p - \epsilon_p'}$, so in the first case, we directly set $\epsilon' = \epsilon  - \log{(1 + \frac{c}{n\Lambda})^2}$, and $\Delta = 0$ if $\epsilon' > 0$. In the second case, we address when $\epsilon'$ becomes negative, and reset $\epsilon_p' = \frac{\epsilon}{2}$ and $\Delta = \frac{c}{n(e^{\frac{\epsilon_p}{4}}- 1)}- \Lambda$. We can verify that falling into the second case implies $\Delta > 0$. Hence,
\begin{align}
\frac{\abs{\det(J(f_{priv} \rightarrow b | D))}^{-1}}{\abs{\det(J(f_{priv} \rightarrow b' | D'))}^{-1}} \le e^{\epsilon_p' - \epsilon} \label{thm9:ratio_jacobian}
\end{align}
Next, we bound the density of the ratio of the noise. From~\eqref{thm9:b}, we have
\begin{align*}
& b' - b = y_n l'(y_n f^T x_n) x_n - y_n' l'(y_n f^T x_n') x_n'
\end{align*}

From the conditions that $\abs{l'} \le 1, \abs{y_i} \le 1, \norm{x_i} \le 1$,
\begin{align*}
& \norm{b} - \norm{b'} \le \norm{b - b'} \le 2
\end{align*}

Then, \begin{align}
 \frac{\mu(b | D)}{\mu(b' | D')} = \frac{e^{-\frac{\epsilon_p' \norm{b}}{2}}}{e^{\frac{-\epsilon' \norm{b'}}{2}}}\le e^{\frac{\epsilon_p'(\norm{b} - \norm{b'})}{2}} \le e^{\epsilon'_p} \label{thm9:ratio_noise}
\end{align}
 Combining the two parts~\eqref{thm9:ratio_jacobian} and~\eqref{thm9:ratio_noise} in~\eqref{thm9:b} completes the proof.
\end{proof}

\subsubsection{Performance Guarantees}
Objective perturbation has a better generalization bound compared to output perturbation.
\begin{theorem} (Details omitted for progress report)
$$n > C\max{\left\{ \frac{\norm{f_0}^2 \log{\frac{1}{\delta}}}{\epsilon_g^2}, \frac{c\norm{f_0}^2}{\epsilon_g \epsilon}, \frac{d\log{\frac{d}{\delta}} \norm{f_0}}{\epsilon_g\epsilon}\right\}}.$$
\end{theorem}

TODO: compare the sample complexity for the two methods in word
\section{Parameter Tuning}
\section{Examples}
In this section, we briefly give examples on how standard learning algorithms fit in this framework.
	\subsection{Logistic Regression}
	In regularized logistic regression, we have
	\begin{align*}
	& N(f) = \frac{1}{2} \norm{f}^2\\
	& l(z) = \log{(1 + e^{-z})}
	\end{align*}

	$l$ is continuous and doubly differentiable with $c = \frac{1}{4}$.
	\subsection{SVM}
	In $L_2$-regularized support vector machines,
	\begin{align*}
	& N(f) = \frac{1}{2} \norm{f}^2\\
	& l(z) = \max{(0, 1 - z)}
	\end{align*}

	This loss function is not differentiable. There are two ways to address the issue:
	\begin{enumerate}[label=(\roman*)]
		\item Use a doubly differentiable surrogate function.
		\item Use a differentiable surrogate function.
		For example, the Huber loss is only piecewise doubly differentiable. We slightly modify the proofs by arguing that the set in which the function is not differentiable has measure $0$, and hence does not affect the density ratio in the definition of differential privacy.
	\end{enumerate}

\section{Conclusion}

{\small
\bibliographystyle{plain}
\bibliography{proposal}
}

\end{document}
