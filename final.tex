\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage[inline]{enumitem}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{cleveref}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09

\title{A Survey on Differential Privacy}

\author{
Achal Dave \\
\texttt{achald@cs.cmu.edu} \\
\And
Jingyan Wang \\
\texttt{jingyanw@cs.cmu.edu} \\
\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\input{commands}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle
%\item What we have done so far:
%
%We read part of the book~\cite{dwork2014algorithmic} for the background and introduction. We read~\cite{chaudhuri2011erm} on differential privacy in machine learning.
%\item What remains to be done:
%
%We plan to scrutinize and summarize the proof ideas in~\cite{chaudhuri2011erm}.
%
%We also plan to survey and read a few more papers. The options include:
%\begin{itemize}
%\item Privacy attack approaches when databases are not protected by differential privacy, \emph{e.g.}~\cite{ganta2008composition}.
%\item Further parts in the book.
%\item Relevant papers from recent conferences.


\section{Introduction}
The growth of the Internet and electronic databases has led to an
increasing concern about the privacy of user information. These databases
facilitate researchers to analyze patterns from large groups of people and develop machine learning algorithms, such as
identifying trends or recommending movies. Although increasingly useful, these methods come at the cost
of user privacy: as we ask questions about a large dataset of users'
information, we receive information of not only the group, but also of
individual users. Prior work has theoretically~\cite{ganta2008composition} and empirically~\cite{sweeney1997weaving} demonstrated that even anonymous datasets can reveal
sensitive details about individuals. For example, \cite{narayanan2008robust} has shown that individual subscribers in the Netflix Prize dataset can be accurately identified with a minimal amount of auxiliary public information. 

Differential privacy offers a formal paradigm which guarantees provable privacy for users, while allowing
queries that reveal aggregated statistics about the dataset as a whole. Revisiting machine learning problems with this notation of differential privacy has been giving rise to lots of new algorithms and theoretical guarantees.

We survey the prior works on differential privacy, with an emphasis on empirical risk minimization (ERM). Section~\ref{overview} introduces the fundamental concepts and basic properties. Section~\ref{section:erm} discusses two algorithms for empirical risk minimization and considers the tradeoffs between privacy guarantees and generalization performance. The parameter tuning technique and specific examples of ERM (\emph{e.g.} logistic regression, SVM) are also discussed.

% Reading List
% Some broad motivation for the need for differential privacy. \cite{narayanan2008robust}
% Fundamentals of differential privacy from excerpts of a book. \cite{dwork2014algorithmic}
% A survey of differential privacy results. \cite{dwork2008differential}
% Applications of differential privacy in empirical risk minimization, which include logistic regression and SVM.~\cite{chaudhuri2011differentially} If time permits, papers from recent conferences.

\section{Overview}\label{overview}
The notion of differential privacy~\cite{dwork2006calibrating} was first developed by Dwork \emph{et al}.

We consider datasets that are collections of records from some
universe $\mathcal{X}$. Further, we can represent such a dataset $x$ as a
\textit{histogram} of records, so that $x \in \mathbb{N}^{\abs{\X}}$. If we want
to collect a dataset where we ask people to say whether they smoke or not, the
universe of responses is $\mathcal{X} = \{0, 1\}$, and the histogram
representation is simply a count of the number of people who smoke and the
number of people who do not smoke.

We would like to answer \textit{queries} from datasets while preserving some
notion of privacy. We let $\mathcal{M}$ be a mechanism for answering
queries. To preserve privacy, we want the response of this mechanism
to have a similar distribution regardless of any one individual's response.
That is, we want that for two datasets $x, y$ that differ in only one
individual (that is, $\norm{x - y}_1 \leq 1$), the distribution of
$\mathcal{M}(x)$ is close to $\mathcal{M}(y)$.

With this notation, we can formalize $(\epsilon, \delta)$-differential privacy.

\begin{definition}{(Differential Privacy)}
A random mechanism $\mathcal{M}$ is $(\epsilon, \delta)$-differentially private
if for all $\mathcal{S} \subseteq \textrm{Range}(\mathcal{M})$ and for all
datasets $x, y \in \mathbb{N}^{\abs{\mathcal{X}}}$
such that $\norm{x - y}_1 \leq 1$,
\begin{align*}
P(\mathcal{M}(x) \in \mathcal{S}) \leq \exp{(\epsilon)} P(\mathcal{M}(y) \in \mathcal{S}) + \delta
\end{align*}
\end{definition}

A simple, intuitive way of ensuring differential privacy is to add noise to the
output of queries. Suppose a query
\[ f(x) : \mathbb{N}^{\abs{\mathcal{X}}} \rightarrow \mathbb{R}^k \]
is requested. Then, instead of
returning $f(X)$, we can return $f(x) + (Y_1, \dots, Y_k)$, where $Y_i$ is noise
drawn from some distribution. However, the noise must depend, in some sense, on
how much $f$ can change for values of $x, y$ close to each other.

We can formalize this as follows. Define $\Delta f$, the $L_1$-sensitivity of
$f$, as follows:
\begin{align*}
\Delta f = \max_{\substack{x, y \in \Nb^{\abs{\Xc}} \\ \norm{x - y}_1 = 1}}
                \norm{f(x) - f(y)}_1
\end{align*}

If $f$ is highly sensitive, then we will need to add more noise to prevent an
adversary from distinguishing $x$ or $y$ based on $f(x)$ and $f(y)$. It turns
out that we can use what is known as the Laplace mechanism to do this.

\begin{definition}{(Laplace Mechanism)}
For any function $f : \Nb^{\abs{\Xc}} \rightarrow \Rb^k$ with $L_1$-sensitivity
$\Delta f$, the Laplace mechanism is defined as
\begin{align*}
\Mc_L(x, f(\cdot), \epsilon) = f(x) + (Y_1, \dots, Y_k)
\end{align*}
where $Y_i$ are i.i.d. drawn from Lap($\Delta f / \epsilon$).
\label{def:laplace_mechanism}
\end{definition}

It can be shown that this mechanism provides $(\epsilon, 0)$ differential
privacy. Note that this mechanism adds noise to the response of the query.

In some cases, adding noise to the response of a query can drastically lower
the utility of the response. In this case, we can instead consider the
exponential mechanism, which allows specifying a utility function for responses.

Define a utility function $u : \Nb^{\abs{\Xc}} \times \Rc \rightarrow \Rb$,
which assigns a utility to a response for a given database. Similar to the
sensitivity of $f$, let's define the sensitivity of $u$:
\begin{align*}
\Delta u \triangleq \max_{r \in \Rc} \max_{x, y : \norm{x - y}_1 \leq 1} \abs{u(x, r) - u(y, r)}
\end{align*}

For this utility function, the exponential mechanism is defined as follows
\begin{definition}{(Exponential Mechanism)}
The exponential mechanism $\Mc_E(x, u, \Rc)$ outputs an element $r \in \Rc$ with
probability proportional to $\exp{(\frac{\epsilon u(x, r)}{2 \Delta u})}$.
\label{def:exponential_mechanism}
\end{definition}

Unlike the Laplace mechanism, the exponential mechanism doesn't add arbitrary
noise to the response; instead, it simply randomizes \textit{which} of a set of
responses to return, so that the utility is not drastically reduced with high
probability. It can be shown that the exponential mechanism preserves
$(\epsilon, 0)$ differential privacy.

\section{Privacy-Preserving Machine Learning}\label{section:erm}
In this section, we discuss how to preserve differential privacy during the learning procedure~\cite{chaudhuri2011erm}.

We consider the standard supervised learning setting, where we aim to learn a mapping $f: \X \rightarrow \Y$ from the training data $D = \{(x_i, y_i) \in \X \times \Y\}_{i=1}^n$. One way to solve for such a function $f$ is through empirical risk minimization (ERM):
$$f^* = \argmin_{f\in \F} R(f, D) \ =\argmin_{f\in \F} \frac{1}{n}\sum_{i=1}^n L(f(x_i), y_i) + \lambda N(f)$$
$\F$ is a function class which $f$ is optimized over. $L:\Y \times \Y \rightarrow \R$ is a loss function. $N(\cdot)$ is a regularizer which penalizes the complexity of $f$. Specifically, we consider the classification problem with a linear predictor $f(x) = f^T x$ where $f\in \R^d$.

The mapping $f$ may reveal certain properties of the training data and lead to privacy leak. Following ideas from Section~\ref{overview}, we add noise to the learning procedure to preserve privacy. We discuss two approaches: (1) \emph{output perturbation}: solve $\hat{f}$ and perturb $\hat{f}$ (2) \emph{objective perturbation}: perturb the objective function $R(f, D)$ and solve for $\hat{f}$ from the perturbed objective.

\subsection{Output Perturbation}
We solve the ERM and perturb the optimizer using the Laplace mechanism. The procedure is given in Algorithm~\ref{alg:output_perturb}.

\begin{algorithm}[htb]
\begin{algorithmic}[1]
  \Function{Output Perturbation}{$D, \epsilon, \lambda$}
    \State Solve $\hat{f} = \argmin_f R(f, D)$.
    \State Draw a random vector $b\in \R^d, \norm{b}\sim \text{Lap}(\frac{1}{\beta})$, where $\beta = \frac{n\lambda \epsilon}{2}$.
    \State Return $f_{p} = \hat{f} + b$
   \EndFunction
\end{algorithmic}
\caption{Output perturbation}\label{alg:output_perturb}
\end{algorithm}

\subsubsection{Privacy Guarantees}
\begin{theorem}
If $N$ is differentiable and $1$-strongly convex; $L$ is convex and differentiable with $\abs{L'} \le 1$, then Algorithm~\ref{alg:output_perturb} is $\epsilon$-differentially private.
\end{theorem}

\begin{proof}
Before touching the details of the proof, it is useful to notice that this method
is very similar to the Laplace mechanism form \Cref{def:laplace_mechanism}.
Let $h(D) = \argmin_f R(f, D)$, which is just a query on a dataset that
returns some value. From \Cref{cor:output_perturb_sensitivity}, we know that
$\Delta_2 h$, the $L_2$ sensitivity of $h$, is $\frac{n \lambda}{2}$. Then,
\Cref{alg:output_perturb} is similar to the Laplace mechanism from
\Cref{def:laplace_mechanism} where the noise is drawn from a
$Lap(\frac{2}{n \lambda \epsilon})$, except that here we rely on the $L_2$
instead of $L_1$ sensitivity, and we draw the magnitude of the noise vector at
random rather than drawing individual elements at random.

Now, we explicitly prove that this method preserves differential privacy.
Consider two datasets $D$ and $D'$ that differ in the value of one individual.
Let $f_1$ and $f_2$ be the solutions output by non-private ERM for datasets $D$
and $D'$ respectively. Let $g(f | D)$ be the density of the solution output by
\Cref{alg:output_perturb} for dataset $D$, and $g(f | D')$ the same for dataset
$D'$.

Since we draw $\norm{b}$ from Lap$(\frac{1}{\beta})$, then the density for
any fixed $b_0$ is proportional to $\exp(-\beta \norm{b_0})$. Then,
\begin{align*}
\frac{g(f | D)}{g(f | D')}
    &= \frac{
         \exp(-\tfrac{n\Lambda\epsilon_p}{2} \norm{b + f_1})
       }{
         \exp(-\tfrac{n\Lambda\epsilon_p}{2} \norm{b + f_2})
       } \\
    &= \exp(-\tfrac{n\Lambda\epsilon_p}{2} (\norm{b + f_2} - \norm{b + f_1})) \\
    &\leq \exp(-\tfrac{n\Lambda\epsilon_p}{2} \norm{f_2 - f_1})
\end{align*}

From \Cref{cor:output_perturb_sensitivity}, we know that the $L_2$ sensitivity
of $\argmin_f R(f, D) \leq \frac{2}{n \Lambda}$, so that
$\norm{f_2 - f_1} \leq \frac{2}{n \Lambda}$. With this, we have satisfied the
definition of $\epsilon_p$ differential privacy:
\begin{align*}
\frac{g(f | D)}{g(f | D')} &\leq \exp(-\epsilon_p),
\end{align*}
concluding the proof.
\end{proof}

\begin{cor}
The $L_2$ sensitivity of $\argmin_f R(f, D)$ is at most $\frac{2}{n \Lambda}$.
\label{cor:output_perturb_sensitivity}
\end{cor}
\begin{proof}
Let $D = \{(x_1, y_1), \dots, (x_n, y_n)\}$ and
$D' = \{(x_1, y_1), \dots, (x_n', y_n')\}$
be two datasets that differ in the value of the $n$th individual.
Let
\begin{align*}
G(f) &= J(f, D) \\
g(f) &= J(f, D') - J(f, D) \\
     &= \frac{1}{n} (L(y_n' f^T x_n') - L(y_n f^T x_n)).
\end{align*}

By the convexity of $L$ and the $1$-strong convexity of $N(\dot)$, we know that
$J(f, D)$ is $\Lambda-$strongly convex, and therefore, so are $G(f)$ and
$G(f) + g(f)$.
Further,
\begin{align*}
\nabla g(f)
= \frac{1}{n} \left(y_n' L'(y_n' f^T x_n') x_n' - y_n L'(y_n f^T x_n) x_n\right)
\end{align*}

Since $y_i \in [-1, 1], \abs{L'(z)} \leq 1$ for all $z$, and
$\norm{x_i} \leq 1$, we can see that for any $f$,
\begin{align*}
\norm{\nabla g(f)} \leq \frac{2}{n}
\end{align*}.

We can now appeal to \Cref{lemma:strong_convex_sensitivity} to conclude the
proof.
\end{proof}

\begin{lemma}
Let $G(f)$ and $g(f)$ be continuous vector-valued functions which are
differentiable at all points. Let $G(f)$ and $G(f) + g(f)$ be $\lambda-$strongly
convex. If $f_1 = \argmin_f G(f)$, and $f_2 = \argmin_f G(f) + g(f)$, then
\begin{align*}
\norm{f_1 - f_2} \leq \frac{1}{\lambda} \max_f \norm{\nabla g(f)}
\end{align*}
\label{lemma:strong_convex_sensitivity}
\end{lemma}

\begin{proof}
TODO(achald): Should we write out this proof?
\end{proof}

\subsubsection{Performance Guarantees}
To analyze the performance guarantees, we compare the loss induced by the output $f_{priv}$ of the algorithm with the loss induced any arbitrary classifier $f_0$. The norm of $f_0$ is included in the sample complexity bound.
\begin{theorem} Let $f_0$ be any classifier. Let $L$ be the true loss. If $l$ is differentiable and continuous with $c$-Lipschitz derivative $l'$ with $\abs{l'} \le 1$. Then, there exists a constant $C$ \emph{s.t.} when the number of training samples $n$ satisfies
$$n > C\max{\left\{\frac{\norm{f_0}^2 \log{\frac{1}{\delta}}}{\epsilon_g^2}, \frac{d\log{\frac{d}{\delta}}\norm{f_0}^2}{\epsilon_g \epsilon}, \frac{d\log{\frac{d}{\delta}}\sqrt{c} \norm{f_0}^2}{\epsilon_g^{\frac{3}{2}}\epsilon}\right\}},$$

the output $f_{priv}$ of Algorithm~\ref{alg:output_perturb} satisfies $P(L(f_{priv}) \le L(f_0) + \epsilon_g) \ge 1 - 2\delta$.
\end{theorem}

\begin{proof}[Proof sketch]
Let $\overline{J}(f) = L(f) + \frac{\Lambda}{2} \norm{f}^2$, and let \begin{align*}
f_{rtr} = \argmin_f \overline{J}(f)\\
f^* = \argmin_f J(f, D)
\end{align*}

We decompose the loss as follows:
\begin{align*}
L(f_{priv}) -  L(f_0) = [\overline{J}(f_{priv}) - \overline{J}(f_{rtr})] + \left[\overline{J}(f_{rtr}) - \overline{J}(f_0)\right] + \frac{\Lambda}{2} \norm{f_0}^2 - \frac{\Lambda}{2} \norm{f_{priv}}^2
\end{align*}

For the first term, we first bound the difference between the objective of $f_*$ and that of the differentially-private solution $f_{priv}$:
\begin{align}
J(f_{priv}, D) - J(f^*, D) \le \frac{8d^2\log^2{(\frac{d}{\delta})} (c + \Lambda)}{\Lambda^2 n^2 \epsilon_p^2}.\label{thm15:emp}
\end{align}

Then, applying Theorem 1 of~\cite{sridharan2008objective}, we have
\begin{align*}
\overline{J}(f_{priv}) - \overline{J}(f_{rtr}) & \le 2 [J(f_{priv}, D) - J(f^*, D)] + O(\frac{\log{(\frac{1}{\delta})}}{\Lambda n})\\
& \le  \frac{16d^2\log^2{(\frac{d}{\delta})} (c + \Lambda)}{\Lambda^2 n^2 \epsilon_p^2} + O(\frac{\log{(\frac{1}{\delta})}}{\Lambda n})
\end{align*}
The second term $\overline{J}(f_{rtr}) - \overline{J}(f_0) \le 0$.

Setting $\Lambda = \frac{\epsilon_g}{\norm{f_0}^2}$, we obtain
\begin{align*}
L(f_{priv}) - L(f_0) \le  \frac{16d^2\log^2{(\frac{d}{\delta})} (c + \Lambda)}{\Lambda^2 n^2 \epsilon_p^2} + O(\frac{\log{(\frac{1}{\delta})}}{\Lambda n}) + \frac{\epsilon_g}{2}.
\end{align*}

Equating the RHS with $\epsilon_g$ and solving for $n$ complete the proof.
\end{proof}

\subsection{Objective Perturbation}
Alternative to perturbing the solution of the algorithm, we can also perturb the objective before optimization. The procedure is given in Algorithm~\ref{alg:obj_perturb}.

\begin{algorithm}[htb]
\begin{algorithmic}[1]
  \Function{Objective Perturbation}{$D, \epsilon, \lambda$}
    \State Set $\epsilon' = \epsilon - \log{(1 + \frac{2c}{n\lambda} + \frac{c^2}{n\lambda^2})}$.
    \If{$\epsilon' > 0$}
	\State $\Delta = 0$.
    \Else
	\State $\epsilon' = \frac{\epsilon}{2}$.
	\State $\Delta = \frac{c}{n(e^{\frac{\epsilon}{4}} - 1)} = \lambda$.
    \EndIf
    \State Draw a random vector $b\in \R^d, \norm{b}\sim \text{Lap}(\frac{1}{\beta})$, where $\beta = \frac{\epsilon'}{2}$.
    \State Return $f_{p} = \argmin_f R(f, D) + \frac{1}{n} b^T f + \frac{1}{2}\Delta\norm{f}^2 $
   \EndFunction
\end{algorithmic}
\caption{Objective perturbation}\label{alg:obj_perturb}
\end{algorithm}

We note that \begin{enumerate*}[label=(\roman*)]
\item There is an additional regularization term $\frac{1}{2}\Delta \norm{f}_2^2$.
\item A slack of $\log{(1 + \frac{2c}{n} + \frac{c^2}{n^2\lambda^2})}$ is introduced. When proving $\epsilon$-differential privacy, we split the density ratio into two factors, and bound them respectively with $e^{\epsilon'}$ and $e^{\epsilon - \epsilon'}$. The two cases are needed to ensure that $\epsilon' > 0$.
\end{enumerate*}

\subsubsection{Privacy Guarantees}
\begin{theorem}
If $N$ is doubly differentiable and $1$-strongly convex; $L$ is convex and doubly differentiable with $\abs{L'} \le 1, \abs{L''} \le c$, then Algorithm~\ref{alg:obj_perturb} is $\epsilon$-differentially private.
\end{theorem}

\begin{proof}[Proof sketch]
Since $J(f, D) + \frac{1}{2}\Delta \norm{f}_2^2$ is strongly-convex, we take the gradient and set it to 0:
\begin{align}
\del [J(f, D) +  \frac{1}{2}\Delta \norm{f}_2^2]= & \frac{1}{n}\sum_{i=1}^n \del l(f(x_i), y_i) + \frac{1}{n} b+ \lambda \del N(f) + \Delta f  \triangleq 0\nonumber\\
b =&  -\sum_{i=1}^n \del l(f^Tx_i, y_i) -n \lambda \del N(f) - n\Delta f\nonumber\\
= & -\sum_{i=1}^n y_i l''(y_i f^T x) x_i - n\Lambda \del N(f) - n\Delta f\label{thm9:b}
\end{align}

This equation defines a mapping from any fixed $f$ to a unique $b$. On the other hand, since the objective is strongly-convex, for any fixed $b$, there exists a unique minimizer $f$. Hence, there is a bijection between $b$ and $f$.

Then, we write the density ratio of the output $f_{priv}$ in terms of the density ratio of the noise $b$, using standard transformation of variables:
\begin{align}
\frac{g(f_{priv} | D)}{g(f_{priv} | D')} = \frac{\mu(b | D)}{\mu(b' | D')} \cdot \frac{\abs{\det(J(f_{priv} \rightarrow b | D))}^{-1}}{\abs{\det(J(f_{priv} \rightarrow b' | D'))}^{-1}}, \label{thm9:main}
\end{align}
where $J$ denotes the Jacobian matrix of the mapping.

From~\eqref{thm9:b}, we analytically compute and bound the Jacobian ratio:
\begin{align*}\frac{\abs{\det(J(f_{priv} \rightarrow b | D))}^{-1}}{\abs{\det(J(f_{priv} \rightarrow b' | D'))}^{-1}} \le \left(1 + \frac{c}{n(\Lambda + \Delta)}\right)^2
\end{align*}

We want to bound this term by $e^{\epsilon_p - \epsilon_p'}$, so in the first case, we directly set $\epsilon' = \epsilon  - \log{(1 + \frac{c}{n\Lambda})^2}$, and $\Delta = 0$ if $\epsilon' > 0$. In the second case, we address when $\epsilon'$ becomes negative, and reset $\epsilon_p' = \frac{\epsilon}{2}$ and $\Delta = \frac{c}{n(e^{\frac{\epsilon_p}{4}}- 1)}- \Lambda$. We can verify that falling into the second case implies $\Delta > 0$. Hence,
\begin{align}
\frac{\abs{\det(J(f_{priv} \rightarrow b | D))}^{-1}}{\abs{\det(J(f_{priv} \rightarrow b' | D'))}^{-1}} \le e^{\epsilon_p' - \epsilon} \label{thm9:ratio_jacobian}
\end{align}
Next, we bound the density of the ratio of the noise. From~\eqref{thm9:b}, we have
\begin{align*}
& b' - b = y_n l'(y_n f^T x_n) x_n - y_n' l'(y_n f^T x_n') x_n'
\end{align*}

From the conditions that $\abs{l'} \le 1, \abs{y_i} \le 1, \norm{x_i} \le 1$,
\begin{align*}
& \norm{b} - \norm{b'} \le \norm{b - b'} \le 2
\end{align*}

Then, \begin{align}
 \frac{\mu(b | D)}{\mu(b' | D')} = \frac{e^{-\frac{\epsilon_p' \norm{b}}{2}}}{e^{\frac{-\epsilon' \norm{b'}}{2}}}\le e^{\frac{\epsilon_p'(\norm{b} - \norm{b'})}{2}} \le e^{\epsilon'_p} \label{thm9:ratio_noise}
\end{align}
 Combining the two parts~\eqref{thm9:ratio_jacobian} and~\eqref{thm9:ratio_noise} in~\eqref{thm9:b} completes the proof.
\end{proof}

\subsubsection{Performance Guarantees}
We follow the same framework as in output perturbation to analyze objective perturbation.
\begin{theorem}
Let $f_0$ be any classifier. If $l$ is convex, doubly differentiable with derivative $|l'| \le 1$ and $|l''(z)| \le c$. Then, there exists a constant $C$ \emph{s.t.} when the number of training samples $n$ satifies
$$n > C\max{\left\{ \frac{\norm{f_0}^2 \log{\frac{1}{\delta}}}{\epsilon_g^2}, \frac{c\norm{f_0}^2}{\epsilon_g \epsilon_p}, \frac{d\log{\frac{d}{\delta}} \norm{f_0}}{\epsilon_g\epsilon}\right\}},$$
the output $f_{priv}$ of Algorithm~\ref{alg:obj_perturb} satisfies $P(L(f_{priv}) \le L(f_0) + \epsilon_g) \ge 1 - 2\delta$.
\end{theorem}
\begin{proof}[Proof sketch]
The proof is similar to the output perturbation case, where we decompose the loss into three terms. The analysis for $J(f_{priv}, D) - J(f^*, D)$ is different (\emph{cf.} Equation~\eqref{thm15:emp}).
\end{proof}

Objective perturbation has a better generalization bound compared to output perturbation.

TODO: compare the sample complexity for the two methods in word
\section{Parameter Tuning}
The above methods assumed that we were provided with a fixed value of $\Lambda$,
the regularization parameter. In practice, however, it is common to perform a
sweep over a set of values of $\Lambda$. Usually, we can do this by training
models with multiple values of $\Lambda$, and evaluating them on a hold-out
validation set and choosing the value that performs best on the hold-out set.
However, this requires running the algorithm multiple times on the same dataset,
which can compromise differential privacy by revealing a relationship between
$\Lambda$ and the output. Instead, we need an end to end differentially private
method for choosing $\Lambda$.

One simple option to fix this problem is to tune $\Lambda$ on a public dataset
where privacy is not of concern. Then, using this tuned $\Lambda$, we can use
\Cref{alg:output_perturb} or \Cref{alg:obj_perturb} to train a classifier in a
differentially private manner.

However, such a dataset is not always available. In this cases, we can use the
exponential mechanism to optimize over possible values of $\Lambda$, as in
\Cref{def:exponential_mechanism}.

\begin{algorithm}[htb]
\begin{algorithmic}[1]
  \Function{Privacy-preserving Parameter Tuning}{$D, \{\Lambda_1, \dots, \Lambda_m\}, \epsilon_p$}
    \State Divide $D$ into $m + 1$ equal portions $D_1, \dots D_{m+1}$, each of
           size $\tfrac{\abs{D}}{m + 1}$.
    \State For each $i = 1, \dots, m$, use a privacy-preserving algorithm
           (such as \Cref{alg:output_perturb} or \Cref{alg:obj_perturb}) on
           $D_i$ with parameters $\Lambda_i$ and $\epsilon_p$ to retrieve an
           output $f_i$.
    \State Evaluate $z_i$, the number of mistakes made by $f_i$ on $D_{m+1}$.
    \State Return $f_i$ with probability
           \begin{align*}
           \frac{e^{-\epsilon_p z_i / 2}}{\sum_{j=1}^m e^{-\epsilon_p z_j / 2}}
           \end{align*}
   \EndFunction
\end{algorithmic}
\caption{Parameter tuning}\label{alg:parameter_tuning}
\end{algorithm}

\Cref{alg:parameter_tuning} splits the dataset $D$ into $m+1$ subsets, and uses
the last subset as a hold out dataset. Crucially, though, the algorithm defines
a \textit{utility} function $u(f_i) = -z_i$ as the negation of the number of
mistakes made by the $i$th minimizer. Notice that the $L_1$ sensitivity of
$u$, $\Delta u$, is $1$.  From this view, the algorithm uses the exponential
mechanism and outputs an $f_i$ with probability proportional to
$\exp(\tfrac{\epsilon_p u(x, r)}{2 \Delta u})$ as in
\Cref{def:exponential_mechanism}, thus satisfying differential privacy.

Further, \cite{chaudhuri2011differentially} show that the classifier output by
the above algorithm does not make many more mistakes than the best of the $m$
classifiers.
\begin{theorem}
Let $z_{\text{min}} = \min_i z_i$, and let $z$ be the number of mistakes made on
$D_{m+1}$ by the classifier output from \Cref{alg:parameter_tuning}. Then, with
probability $1 - \delta$,
\begin{align*}
z \leq z_{\text{min}} + \frac{2 \log(m/\delta)}{\epsilon_p}
\end{align*}
\end{theorem}
\section{Examples}
In this section, we briefly give examples on how standard learning algorithms fit in this framework.
	\subsection{Logistic Regression}
	In regularized logistic regression, we have
	\begin{align*}
	& N(f) = \frac{1}{2} \norm{f}^2\\
	& l(z) = \log{(1 + e^{-z})}
	\end{align*}

	$l$ is continuous and doubly differentiable with $c = \frac{1}{4}$.
	\subsection{SVM}
	In $L_2$-regularized support vector machines,
	\begin{align*}
	& N(f) = \frac{1}{2} \norm{f}^2\\
	& l(z) = \max{(0, 1 - z)}
	\end{align*}

	This loss function is not differentiable. There are two ways to address the issue:
	\begin{enumerate}[label=(\roman*)]
		\item Use a doubly differentiable surrogate function.
		\item Use a differentiable surrogate function.
		For example, the Huber loss is only piecewise doubly differentiable. We slightly modify the proofs by arguing that the set in which the function is not differentiable has measure $0$, and hence does not affect the density ratio in the definition of differential privacy.
	\end{enumerate}

\section{Discussions and Conclusion}

We have discussed the definition of differential privacy as well as a couple of
ways to ensure differential privacy for general queries. Further, we showed the
applications of these mechanisms for Empirical Risk Minimization, so that we
are able to train classifiers on datasets in a differentially private manner.
As we look further into providing privacy in the real world, we must consider
additional challenges as well as alternative definitions of privacy.

\textbf{Local Differential Privacy.} So far, we have considered differential
privacy where a trusted authority has access to the true dataset. This authority
can then receive a query, run a differentially private algorithm on the dataset,
and provide some perturbed result of the query to the statistician. In the real
world, however, individuals may be unwilling to sacrifice their privacy to
\textit{any} trusted authority. How can an authority receive some information
from individuals that maintains their privacy from the authority? This is a
stronger claim than differential privacy -- intuitively, if the data is private
from the authority, the authority cannot provide any more privacy sacrificing
information to a statistician who queries the dataset. \cite{duchi2013local}
discuss this setting of \textit{local} differential privacy, and, critically,
show that for many statistical procedures, the effective sample size under
$\alpha$-local differential privacy with $n$ data points in $d$ dimensions is
reduced to $n \alpha^2 / d$. This reduction can lead to drastically poor
estimation results, and shows that providing local differential privacy can be
very costly in terms of the accuracy of query results.

\textbf{Implementation Challenges.} We have seen algorithms that provide
strong theoretical guarantees on both the privacy afforded to the individuals in
a dataset as well and on the accuracy of the output. A number of packages have
been implemented that expose the Laplace mechanism for end users, such as
Airavat \cite{mohan2012gupt} and GUPT \cite{roy2010airavat}. In practice,
however, implementation challenges can sacrifice privacy despite theoretical
guarantees. For example, the details of the floating point standard can cause
severe issues in privacy guarantees. In particular, the non-uniform spacing of
the doubles causes issues in the support of the perturbed output values of the
Laplace mechanism - the algorithm's output may have support on certain values
when run on one dataset, but not on another dataset, violating differential
privacy.  \cite{mironov2012significance} describes this issue, and shows that it
is possible to consistently violate privacy using current implementations of
Laplace mechanisms, and shows how to overcome these challenges.

\textbf{Conclusion.} Despite these challenges facing privacy, it is possible to
provide strong theoretical guarantees in the case of standard differential
privacy for both privacy and performance. This survey, in addition to providing
an overview of differential privacy, discusses methods for performing empirical
risk minimization while maintaining privacy. It remains unclear whether these
methods are optimal, and minimax rates for ERM under differential privacy remain
elusive. However, the methods discussed here show that it is possible to perform
private ERM without significantly sacrificing performance, and suggests that
differential privacy in conjunction with other machine learning problems can
provide a useful trade-off between privacy and utility. Further work in
combining machine learning algorithms with differential privacy will hopefully
alleviate this issue, making it easier for dataset owners (such as companies) to
share useful information about populations with outside groups while maintaining
privacy.

{\small
\bibliographystyle{plain}
\bibliography{proposal}
}
\appendix
\section*{Appendix}
% Division of work:
%We both read the papers above. AD wrote Section 2; JW wrote Section 3.

\end{document}
